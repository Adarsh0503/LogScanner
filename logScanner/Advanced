#!/usr/bin/env python3
import os
import re
import mmap
import argparse
import multiprocessing
import tempfile
import shutil
from datetime import datetime
from functools import partial

def scan_file_with_mmap(file_path, search_parameter):
    """
    Scan a single file using memory-mapped I/O for efficiency.
    Returns a list of matching lines.
    """
    matches = []
    try:
        with open(file_path, 'r') as f:
            # Memory map the file for faster access
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                # Convert to bytes for mmap searching
                search_bytes = search_parameter.encode('utf-8')
                
                # Start position for searching
                current_pos = 0
                
                # Find each occurrence
                while True:
                    found_pos = mm.find(search_bytes, current_pos)
                    if found_pos == -1:
                        break
                    
                    # Find the start of the line containing match
                    line_start = mm.rfind(b'\n', 0, found_pos)
                    if line_start == -1:  # If not found, start of file
                        line_start = 0
                    else:
                        line_start += 1  # Skip the newline character
                    
                    # Find the end of the line
                    line_end = mm.find(b'\n', found_pos)
                    if line_end == -1:  # If not found, end of file
                        line_end = mm.size()
                    
                    # Extract the line and decode to string
                    line = mm[line_start:line_end].decode('utf-8', errors='replace')
                    matches.append(line)
                    
                    # Move to position after current match
                    current_pos = found_pos + 1
                    
    except Exception as e:
        print(f"Error processing {file_path}: {str(e)}")
    
    return file_path, matches

def process_file_generator(file_path, search_parameter):
    """
    Process a file using generators for memory efficiency.
    Alternative to mmap for certain cases.
    """
    matches = []
    try:
        with open(file_path, 'r') as f:
            for line in f:
                if search_parameter in line:
                    matches.append(line.rstrip('\n'))
    except Exception as e:
        print(f"Error processing {file_path}: {str(e)}")
    
    return file_path, matches

def write_results_to_split_files(results, base_output_file, max_file_size_mb=1):
    """
    Write results to multiple files, splitting when they exceed the specified size.
    Includes rollback mechanism to ensure file integrity.
    
    Args:
        results: List of tuples (file_path, matches)
        base_output_file: Base name for output files
        max_file_size_mb: Maximum size of each output file in MB
    
    Returns:
        list: List of generated output files
    """
    max_file_size_bytes = max_file_size_mb * 1024 * 1024
    output_files = []
    total_matches = 0
    
    # Create a temporary directory for rollback
    temp_dir = tempfile.mkdtemp()
    current_file_idx = 1
    current_output_file = f"{base_output_file}.part{current_file_idx:03d}"
    output_files.append(current_output_file)
    
    try:
        # Open the first output file
        with open(current_output_file, 'w') as out_file:
            current_file_size = 0
            
            # Process all matches from all files
            for file_path, matches in results:
                if matches:
                    file_header = f"\n{'=' * 80}\n"
                    file_header += f"MATCHES FROM: {file_path}\n"
                    file_header += f"{'=' * 80}\n\n"
                    
                    # Check if adding this file's matches would exceed the size limit
                    # If so, start a new file
                    if current_file_size + len(file_header) > max_file_size_bytes:
                        out_file.close()
                        
                        current_file_idx += 1
                        current_output_file = f"{base_output_file}.part{current_file_idx:03d}"
                        output_files.append(current_output_file)
                        out_file = open(current_output_file, 'w')
                        current_file_size = 0
                    
                    # Write file header
                    out_file.write(file_header)
                    current_file_size += len(file_header)
                    
                    file_matches = 0
                    for line in matches:
                        line_with_newline = line + '\n'
                        line_size = len(line_with_newline)
                        
                        # If this single line would exceed the file size limit on its own,
                        # we have to include it anyway (can't split a single line)
                        # Otherwise, check if adding this line would exceed the limit
                        if line_size > max_file_size_bytes:
                            # Line is too big for a single file, but we have to include it
                            out_file.write(line_with_newline)
                            file_matches += 1
                            current_file_size += line_size
                        elif current_file_size + line_size > max_file_size_bytes:
                            # This line would exceed the limit, so start a new file
                            # Write file summary for current file
                            summary = f"\nTotal matches in this file: {file_matches}\n\n"
                            out_file.write(summary)
                            out_file.close()
                            
                            # Create new file
                            current_file_idx += 1
                            current_output_file = f"{base_output_file}.part{current_file_idx:03d}"
                            output_files.append(current_output_file)
                            out_file = open(current_output_file, 'w')
                            
                            # Write file header again in the new file
                            out_file.write(file_header)
                            current_file_size = len(file_header)
                            file_matches = 0
                            
                            # Now write the line to the new file
                            out_file.write(line_with_newline)
                            file_matches += 1
                            current_file_size += line_size
                        else:
                            # Line fits in current file
                            out_file.write(line_with_newline)
                            file_matches += 1
                            current_file_size += line_size
                    
                    # Write file summary
                    summary = f"\nTotal matches in this file: {len(matches)}\n\n"
                    out_file.write(summary)
                    current_file_size += len(summary)
                    total_matches += len(matches)
        
        # Create an index file
        index_file = f"{base_output_file}.index"
        with open(index_file, 'w') as idx_file:
            idx_file.write(f"LOG SCAN RESULTS INDEX\n")
            idx_file.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            idx_file.write(f"Total matches found: {total_matches}\n")
            idx_file.write(f"Split into {len(output_files)} files\n\n")
            
            for i, output_file in enumerate(output_files):
                idx_file.write(f"Part {i+1}: {os.path.basename(output_file)}\n")
        
        output_files.append(index_file)
        
    except Exception as e:
        # Rollback: delete partially written files
        print(f"Error writing output files: {str(e)}")
        for file in output_files:
            if os.path.exists(file):
                os.remove(file)
        output_files = []
        raise
    
    finally:
        # Clean up temporary directory
        shutil.rmtree(temp_dir)
    
    return output_files

def scan_logs_parallel(directory_path, search_parameter, output_file=None, use_mmap=True, num_processes=None, max_file_size_mb=1):
    """
    Scan all log files in the directory in parallel for lines containing the search parameter
    and combine them into a single output file or multiple files if size threshold is reached.
    
    Args:
        directory_path (str): Path to the directory containing log files
        search_parameter (str): Text to search for in log files
        output_file (str, optional): Path to output file. If None, generates a default name.
        use_mmap (bool): Whether to use mmap for file processing
        num_processes (int, optional): Number of processes to use. If None, uses CPU count.
        max_file_size_mb (int): Maximum size of each output file in MB
    
    Returns:
        str or list: Path to the output file or list of output files
    """
    # Generate default output filename if not provided
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_output_file = f"combined_logs_{search_parameter}_{timestamp}"
    else:
        # Remove file extension if provided
        base_output_file = os.path.splitext(output_file)[0]
    
    # Collect all log files
    log_files = []
    for root, _, files in os.walk(directory_path):
        for file in files:
            if file.endswith(".log"):
                log_files.append(os.path.join(root, file))
    
    print(f"Found {len(log_files)} log files to scan")
    
    # Determine number of processes
    if num_processes is None:
        num_processes = min(multiprocessing.cpu_count(), len(log_files))
    
    # Choose the processing function
    process_func = scan_file_with_mmap if use_mmap else process_file_generator
    
    # Process files in parallel
    print(f"Processing with {num_processes} processes {'using mmap' if use_mmap else 'using generators'}")
    
    with multiprocessing.Pool(processes=num_processes) as pool:
        # Create a partial function with the search parameter
        partial_func = partial(process_func, search_parameter=search_parameter)
        
        # Process all files and collect results
        results = pool.map(partial_func, log_files)
    
    # Write results to split output files
    try:
        output_files = write_results_to_split_files(results, base_output_file, max_file_size_mb)
        
        # Count total matches
        total_matches = sum(len(matches) for _, matches in results if matches)
        
        if len(output_files) > 1:
            print(f"Scanning complete. Found {total_matches} matches across all files.")
            print(f"Results split into {len(output_files)-1} files with index at {output_files[-1]}")
            return output_files
        else:
            print(f"Scanning complete. Found {total_matches} matches across all files.")
            print(f"Results saved to: {output_files[0]}")
            return output_files[0]
            
    except Exception as e:
        print(f"Error writing output: {str(e)}")
        return None

def main():
    # Set up command line argument parsing
    parser = argparse.ArgumentParser(
        description="Scan log files for a specific parameter and combine matching lines."
    )
    parser.add_argument(
        "search_parameter", 
        help="Text to search for in the log files"
    )
    parser.add_argument(
        "directory_path", 
        help="Path to the directory containing log files"
    )
    parser.add_argument(
        "-o", "--output", 
        help="Output file path (optional)",
        default=None
    )
    parser.add_argument(
        "--no-mmap",
        action="store_true",
        help="Disable memory-mapped I/O (use for very large files or limited memory)"
    )
    parser.add_argument(
        "-p", "--processes",
        type=int,
        default=None,
        help="Number of processes to use (default: number of CPU cores)"
    )
    parser.add_argument(
        "-r", "--regex",
        action="store_true",
        help="Use regex pattern matching instead of simple string search"
    )
    parser.add_argument(
        "-s", "--size",
        type=int,
        default=1,
        help="Maximum size of each output file in MB (default: 1)"
    )
    
    args = parser.parse_args()
    
    # If regex is requested, use the original regex-based function
    if args.regex:
        # Import the original regex-based function (assumed to be defined elsewhere)
        # scan_logs(args.directory_path, args.search_parameter, args.output)
        print("Regex-based scanning requested - this would use the regex implementation")
        return
    
    # Call the optimized scan_logs function with provided arguments
    scan_logs_parallel(
        args.directory_path, 
        args.search_parameter, 
        args.output,
        use_mmap=not args.no_mmap,
        num_processes=args.processes,
        max_file_size_mb=args.size
    )

if __name__ == "__main__":
    main()
